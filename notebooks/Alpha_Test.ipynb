{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c4434b-d6bf-41f3-a314-14f98d4994af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e81f836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd8f76",
   "metadata": {},
   "source": [
    "#### Configure path parameters and read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eba28e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), *['..'] * 1)) \n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, \"data\", \"datasets\", \"aircraft\", \"images\")\n",
    "\n",
    "RADOM_SEED = 2020\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4190f25-b81b-4f7f-a38a-5a07c5636bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/mids/m250420/Capstone/lc-14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9906ced9-a9a1-46f4-86ed-7f8572b47fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mids/m250420/Capstone/lc-14/data/datasets/aircraft/images'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fbf458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_dir = IMAGES_DIR\n",
    "labels_fp = os.path.join(ROOT_DIR, \"data\", \"datasets\", \"aircraft\", \"annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f27dc56-fe8a-4c76-a36c-723e887b289b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mids/m250420/Capstone/lc-14/data/datasets/aircraft/annotations.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29963c",
   "metadata": {},
   "source": [
    "#### Load utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30a4905-25a7-4c57-a20f-b93410c179f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f88d0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.utilities.data.aircraft_dataloader import get_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e0e27",
   "metadata": {},
   "source": [
    "#### Define a minimal transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68730d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformations = A.Compose([\n",
    "    A.Resize(256, 256),  # This transform resizes each image before subsequent processing\n",
    "    A.HorizontalFlip(p=0.5),  # This transform flips images horizontally with a 50% probability\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # This transformation normalizes from standard RGB to grayscale\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "  \n",
    "    ToTensorV2(),  # This transform maps our image to a torch.Tensor object\n",
    "])\n",
    "## play around with size -- 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bab02",
   "metadata": {},
   "source": [
    "#### Building a `torch.Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87a8871f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "dataloader = get_dataloader(\n",
    "  image_dir=image_dir,\n",
    "  labels_fp=labels_fp,\n",
    "  transformations=transformations,\n",
    "  mode='train',\n",
    "  train_frac=TRAIN_FRAC,\n",
    "  val_frac=VAL_FRAC,\n",
    "  seed=RADOM_SEED,\n",
    "  batch_size=4,\n",
    "  shuffle=True,\n",
    "  num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56196a",
   "metadata": {},
   "source": [
    "#### Building a baseline MLP using `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f66f06fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineMLP, self).__init__()\n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolution layer with more filters\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fourth convolution layer (new addition)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)  # Adjust for the feature map size after pooling\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.bn3(self.conv3(x)))  # Apply batch normalization after 3rd convolution\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))  # Apply 4th convolution layer and pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten dynamically for batch size\n",
    "        x = self.relu5(self.fc1(x))  # Fully connected layer\n",
    "        x = self.fc2(x)  # Second fully connected layer\n",
    "        x = self.fc3(x)  # Final prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c85278",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "087c291b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic hyperparameters\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "947ac355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 14.6875\n",
      "Epoch [2/10], Loss: 13.6323\n",
      "Epoch [3/10], Loss: 11.7460\n",
      "Epoch [4/10], Loss: 21.0803\n",
      "Epoch [5/10], Loss: 19.6403\n",
      "Epoch [6/10], Loss: 5.2284\n",
      "Epoch [7/10], Loss: 8.0623\n",
      "Epoch [8/10], Loss: 13.3601\n",
      "Epoch [9/10], Loss: 9.0751\n",
      "Epoch [10/10], Loss: 7.3661\n"
     ]
    }
   ],
   "source": [
    "baseline_mlp_model = BaselineMLP()\n",
    "criterion = nn.SmoothL1Loss(beta=1.0)  \n",
    "#criterion = nn.MSELoss()  Mean Squared Error for regression\n",
    "optimizer = torch.optim.Adam(baseline_mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (images, targets) in enumerate(dataloader):\n",
    "    if i == len(dataloader) - 1: continue  # save the last batch for demonstration\n",
    "    # Forward pass\n",
    "    outputs = baseline_mlp_model(images)\n",
    "    loss = criterion(outputs.squeeze(), targets)  # Ensure outputs are squeezed to match counts shape\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1e464",
   "metadata": {},
   "source": [
    "Check the model's prediction on the validation set for a `torch.Dataset` with the same `train_frac`, `val_frac`, and `seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d595a83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "val_dataloader = get_dataloader(\n",
    "  image_dir,\n",
    "  labels_fp,\n",
    "  transformations=transformations,\n",
    "  mode='val',\n",
    "  train_frac=TRAIN_FRAC,\n",
    "  val_frac=VAL_FRAC,\n",
    "  seed=RADOM_SEED,\n",
    "  batch_size=1,\n",
    "  shuffle=False,\n",
    "  num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09256240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val image 1, predicted count: 31.1367, true count: 31.0000\n",
      "val image 2, predicted count: 31.6776, true count: 24.0000\n",
      "val image 3, predicted count: 29.9612, true count: 19.0000\n",
      "val image 4, predicted count: 31.3931, true count: 49.0000\n",
      "val image 5, predicted count: 29.4936, true count: 27.0000\n",
      "val image 6, predicted count: 29.3425, true count: 15.0000\n",
      "val image 7, predicted count: 30.2501, true count: 30.0000\n",
      "val image 8, predicted count: 30.4587, true count: 52.0000\n",
      "val image 9, predicted count: 31.7971, true count: 39.0000\n",
      "val image 10, predicted count: 31.8876, true count: 26.0000\n",
      "Mean absolute error: 8.8100\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i, (val_images, targets) in enumerate(val_dataloader):\n",
    "  predicted_counts = baseline_mlp_model(val_images)\n",
    "  # We validate based on the mean absolute error\n",
    "  losses.append(torch.abs(predicted_counts - targets).item())\n",
    "  print(f\"val image {i+1}, predicted count: {predicted_counts.item():.4f}, true count: {targets.item():.4f}\")\n",
    "\n",
    "mean_loss = sum(losses) / len(losses)\n",
    "print(f\"Mean absolute error: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e798cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to github -- TestModel\n",
    "# pull dataloader to TestModel notebook -- use that to test\n",
    "# alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
